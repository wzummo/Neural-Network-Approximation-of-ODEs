{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRuv_ST-0Vml"
      },
      "source": [
        "# Consider the ODE <br>\n",
        "$dy/dx = -2xy$ <br> \n",
        "with initial condition <br>\n",
        "$y(0)=1$ <br>\n",
        "and analytical solution <br>\n",
        "$y(x)=e^{-x^2}$\n",
        "\n",
        "following matlab example https://www.mathworks.com/help/deeplearning/ug/solve-odes-using-a-neural-network.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tpvcLC8uttq5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-y7bRmz9wFub"
      },
      "outputs": [],
      "source": [
        "def MSE(y_pred, y_true=None):\n",
        "    \"\"\"\n",
        "    computes MSE error\n",
        "    \"\"\"\n",
        "    if y_true is None:\n",
        "        return tf.reduce_mean(y_pred ** 2)\n",
        "    else:\n",
        "        return tf.reduce_mean((y_pred - y_true) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6bwKF3O0t1RX"
      },
      "outputs": [],
      "source": [
        "# take n number of samples\n",
        "n_samples = 10000\n",
        "\n",
        "# sample x from 0 to 2\n",
        "x = np.linspace(0,2,n_samples).reshape(n_samples,1)\n",
        "\n",
        "# initial condition of x is when x=0\n",
        "x0 = np.zeros(n_samples,).reshape(n_samples,1)\n",
        "\n",
        "# initial conditoin of y(0)=1\n",
        "y0 = np.ones(n_samples).reshape(n_samples,1)\n",
        "\n",
        "# move these numpy arrays to tensors\n",
        "x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "x0_tensor = tf.convert_to_tensor(x0, dtype=tf.float32)\n",
        "y0_tensor = tf.convert_to_tensor(y0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_tensor, x0_tensor, y0_tensor))\n",
        "\n",
        "batch_size = 1000\n",
        "train_dataset_shuffled = train_dataset.shuffle(10000).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o_YtmsG5uCi-"
      },
      "outputs": [],
      "source": [
        "# define model -> input x and predict y with one hidden layer\n",
        "\n",
        "# accept 1 input \n",
        "inputs = tf.keras.Input((1,))\n",
        "\n",
        "# two hidden layer with 20 neurons, activation function of sigmoid and no bias \n",
        "L1 = tf.keras.layers.Dense(30, activation='sigmoid', use_bias=True)(inputs)\n",
        "\n",
        "# output layer predicts y with activation function of sigmoid to constrain output between 0 and 1\n",
        "outputs = tf.keras.layers.Dense(1)(L1)\n",
        "\n",
        "# build model\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "num_epochs = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ic3G-rBKygpN"
      },
      "outputs": [],
      "source": [
        "# define loss function \n",
        "def loss_ode(model, x_tensor, x0_tensor, y0_tensor):\n",
        "  x_copy = tf.identity(x_tensor)                       # make copy of x for gradients\n",
        "  with tf.GradientTape() as t:\n",
        "    t.watch(x_copy)                                    # watch x variable for auto diff\n",
        "    y_pred = model(x_copy)                             # make model prediction for y\n",
        "  dy_dx = t.gradient(y_pred,x_copy)                    # take derivative dy/dx\n",
        "  res = dy_dx + 2*y_pred*x_copy                        # define resisual from the ode \n",
        "  res_loss = sum([MSE(res_i) for res_i in res])        # calculate MSE loss for residual\n",
        "\n",
        "  ic_loss = sum([MSE(model(x0_tensor),y0_tensor)])     # calculate initial condition loss comparing the model\n",
        "                                                       # evaluated at 0 to the expected result of 1\n",
        "\n",
        "  ic_coeff = 5                                         # initial condition coefficient taken from matlab example\n",
        "  \n",
        "  loss_full = res_loss + ic_coeff*ic_loss              # calcuate total loss\n",
        "\n",
        "  return loss_full, res_loss, ic_loss                  # return all the lossses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfbiFbolwtsL",
        "outputId": "f481a46f-e57d-4ed2-ba21-9b41d38c1b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, Best Loss = 22.67992401123047\n",
            "epoch: 1, Best Loss = 22.67992401123047\n",
            "epoch: 2, Best Loss = 22.67992401123047\n",
            "epoch: 3, Best Loss = 21.56422233581543\n",
            "epoch: 4, Best Loss = 14.996681213378906\n",
            "epoch: 5, Best Loss = 13.91118049621582\n",
            "epoch: 6, Best Loss = 11.779365539550781\n",
            "epoch: 7, Best Loss = 10.940547943115234\n",
            "epoch: 8, Best Loss = 9.190561294555664\n",
            "epoch: 9, Best Loss = 8.975481986999512\n",
            "epoch: 10, Best Loss = 7.909783363342285\n",
            "epoch: 11, Best Loss = 7.246533393859863\n",
            "epoch: 12, Best Loss = 6.760617256164551\n",
            "epoch: 13, Best Loss = 6.170521259307861\n",
            "epoch: 14, Best Loss = 5.807347774505615\n",
            "epoch: 15, Best Loss = 5.606633186340332\n",
            "epoch: 16, Best Loss = 5.416720867156982\n",
            "epoch: 17, Best Loss = 5.219606399536133\n",
            "epoch: 18, Best Loss = 5.149991989135742\n",
            "epoch: 19, Best Loss = 5.069460868835449\n",
            "epoch: 20, Best Loss = 4.979179382324219\n",
            "epoch: 21, Best Loss = 4.913604259490967\n",
            "epoch: 22, Best Loss = 4.904028415679932\n",
            "epoch: 23, Best Loss = 4.857002258300781\n",
            "epoch: 24, Best Loss = 4.839352130889893\n",
            "epoch: 25, Best Loss = 4.839352130889893\n",
            "epoch: 26, Best Loss = 4.8328704833984375\n",
            "epoch: 27, Best Loss = 4.828179836273193\n",
            "epoch: 28, Best Loss = 4.8245954513549805\n",
            "epoch: 29, Best Loss = 4.809733867645264\n",
            "epoch: 30, Best Loss = 4.809733867645264\n",
            "epoch: 31, Best Loss = 4.809733867645264\n",
            "epoch: 32, Best Loss = 4.805624485015869\n",
            "epoch: 33, Best Loss = 4.805004596710205\n",
            "epoch: 34, Best Loss = 4.805004596710205\n",
            "epoch: 35, Best Loss = 4.805004596710205\n",
            "epoch: 36, Best Loss = 4.801741600036621\n",
            "epoch: 37, Best Loss = 4.787755966186523\n",
            "epoch: 38, Best Loss = 4.787755966186523\n",
            "epoch: 39, Best Loss = 4.787755966186523\n",
            "epoch: 40, Best Loss = 4.787755966186523\n",
            "epoch: 41, Best Loss = 4.787755966186523\n",
            "epoch: 42, Best Loss = 4.787755966186523\n",
            "epoch: 43, Best Loss = 4.787755966186523\n",
            "epoch: 44, Best Loss = 4.787755966186523\n",
            "epoch: 45, Best Loss = 4.784762382507324\n",
            "epoch: 46, Best Loss = 4.784762382507324\n",
            "epoch: 47, Best Loss = 4.784762382507324\n",
            "epoch: 48, Best Loss = 4.784762382507324\n",
            "epoch: 49, Best Loss = 4.784762382507324\n",
            "epoch: 50, Best Loss = 4.784762382507324\n",
            "epoch: 51, Best Loss = 4.784762382507324\n",
            "epoch: 52, Best Loss = 4.784762382507324\n",
            "epoch: 53, Best Loss = 4.784762382507324\n",
            "epoch: 54, Best Loss = 4.784762382507324\n",
            "epoch: 55, Best Loss = 4.783650875091553\n",
            "epoch: 56, Best Loss = 4.783650875091553\n",
            "epoch: 57, Best Loss = 4.783650875091553\n",
            "epoch: 58, Best Loss = 4.783650875091553\n",
            "epoch: 59, Best Loss = 4.776442527770996\n",
            "epoch: 60, Best Loss = 4.770423889160156\n",
            "epoch: 61, Best Loss = 4.770423889160156\n",
            "epoch: 62, Best Loss = 4.770423889160156\n",
            "epoch: 63, Best Loss = 4.770423889160156\n",
            "epoch: 64, Best Loss = 4.770423889160156\n",
            "epoch: 65, Best Loss = 4.770423889160156\n",
            "epoch: 66, Best Loss = 4.770423889160156\n",
            "epoch: 67, Best Loss = 4.770423889160156\n",
            "epoch: 68, Best Loss = 4.770423889160156\n",
            "epoch: 69, Best Loss = 4.767143726348877\n",
            "epoch: 70, Best Loss = 4.767143726348877\n",
            "epoch: 71, Best Loss = 4.767143726348877\n",
            "epoch: 72, Best Loss = 4.767143726348877\n",
            "epoch: 73, Best Loss = 4.763321399688721\n",
            "epoch: 74, Best Loss = 4.763321399688721\n",
            "epoch: 75, Best Loss = 4.756802082061768\n",
            "epoch: 76, Best Loss = 4.756802082061768\n",
            "epoch: 77, Best Loss = 4.756802082061768\n",
            "epoch: 78, Best Loss = 4.756802082061768\n",
            "epoch: 79, Best Loss = 4.755019187927246\n",
            "epoch: 80, Best Loss = 4.755019187927246\n",
            "epoch: 81, Best Loss = 4.753014087677002\n",
            "epoch: 82, Best Loss = 4.753014087677002\n",
            "epoch: 83, Best Loss = 4.753014087677002\n",
            "epoch: 84, Best Loss = 4.753014087677002\n",
            "epoch: 85, Best Loss = 4.749617576599121\n",
            "epoch: 86, Best Loss = 4.749617576599121\n",
            "epoch: 87, Best Loss = 4.738983154296875\n",
            "epoch: 88, Best Loss = 4.738983154296875\n",
            "epoch: 89, Best Loss = 4.733767509460449\n",
            "epoch: 90, Best Loss = 4.733767509460449\n",
            "epoch: 91, Best Loss = 4.733767509460449\n",
            "epoch: 92, Best Loss = 4.733767509460449\n",
            "epoch: 93, Best Loss = 4.733767509460449\n",
            "epoch: 94, Best Loss = 4.733767509460449\n",
            "epoch: 95, Best Loss = 4.733767509460449\n",
            "epoch: 96, Best Loss = 4.733767509460449\n",
            "epoch: 97, Best Loss = 4.732929229736328\n",
            "epoch: 98, Best Loss = 4.732929229736328\n",
            "epoch: 99, Best Loss = 4.732929229736328\n",
            "epoch: 100, Best Loss = 4.732929229736328\n",
            "epoch: 101, Best Loss = 4.732929229736328\n",
            "epoch: 102, Best Loss = 4.732929229736328\n",
            "epoch: 103, Best Loss = 4.730653762817383\n",
            "epoch: 104, Best Loss = 4.728631973266602\n",
            "epoch: 105, Best Loss = 4.728631973266602\n",
            "epoch: 106, Best Loss = 4.726187229156494\n",
            "epoch: 107, Best Loss = 4.726187229156494\n",
            "epoch: 108, Best Loss = 4.715682029724121\n",
            "epoch: 109, Best Loss = 4.715682029724121\n",
            "epoch: 110, Best Loss = 4.715682029724121\n",
            "epoch: 111, Best Loss = 4.714934349060059\n",
            "epoch: 112, Best Loss = 4.714934349060059\n",
            "epoch: 113, Best Loss = 4.714934349060059\n",
            "epoch: 114, Best Loss = 4.714934349060059\n",
            "epoch: 115, Best Loss = 4.711759567260742\n",
            "epoch: 116, Best Loss = 4.711759567260742\n",
            "epoch: 117, Best Loss = 4.711759567260742\n",
            "epoch: 118, Best Loss = 4.711759567260742\n",
            "epoch: 119, Best Loss = 4.711759567260742\n",
            "epoch: 120, Best Loss = 4.711759567260742\n",
            "epoch: 121, Best Loss = 4.70610237121582\n",
            "epoch: 122, Best Loss = 4.70610237121582\n",
            "epoch: 123, Best Loss = 4.70610237121582\n",
            "epoch: 124, Best Loss = 4.70610237121582\n",
            "epoch: 125, Best Loss = 4.70610237121582\n",
            "epoch: 126, Best Loss = 4.7046427726745605\n",
            "epoch: 127, Best Loss = 4.699178695678711\n",
            "epoch: 128, Best Loss = 4.699178695678711\n",
            "epoch: 129, Best Loss = 4.697729110717773\n",
            "epoch: 130, Best Loss = 4.697729110717773\n",
            "epoch: 131, Best Loss = 4.69732141494751\n",
            "epoch: 132, Best Loss = 4.69732141494751\n",
            "epoch: 133, Best Loss = 4.691959381103516\n",
            "epoch: 134, Best Loss = 4.691959381103516\n",
            "epoch: 135, Best Loss = 4.691959381103516\n",
            "epoch: 136, Best Loss = 4.691959381103516\n",
            "epoch: 137, Best Loss = 4.691959381103516\n",
            "epoch: 138, Best Loss = 4.683698654174805\n",
            "epoch: 139, Best Loss = 4.683698654174805\n",
            "epoch: 140, Best Loss = 4.676560401916504\n",
            "epoch: 141, Best Loss = 4.676560401916504\n",
            "epoch: 142, Best Loss = 4.67644739151001\n",
            "epoch: 143, Best Loss = 4.67644739151001\n",
            "epoch: 144, Best Loss = 4.67644739151001\n",
            "epoch: 145, Best Loss = 4.67644739151001\n",
            "epoch: 146, Best Loss = 4.675387859344482\n",
            "epoch: 147, Best Loss = 4.675387859344482\n",
            "epoch: 148, Best Loss = 4.675387859344482\n",
            "epoch: 149, Best Loss = 4.67147970199585\n",
            "epoch: 150, Best Loss = 4.67147970199585\n",
            "epoch: 151, Best Loss = 4.67147970199585\n",
            "epoch: 152, Best Loss = 4.67147970199585\n",
            "epoch: 153, Best Loss = 4.669748306274414\n",
            "epoch: 154, Best Loss = 4.669748306274414\n",
            "epoch: 155, Best Loss = 4.668438911437988\n",
            "epoch: 156, Best Loss = 4.66686487197876\n",
            "epoch: 157, Best Loss = 4.66686487197876\n",
            "epoch: 158, Best Loss = 4.66686487197876\n",
            "epoch: 159, Best Loss = 4.662250995635986\n",
            "epoch: 160, Best Loss = 4.656842231750488\n",
            "epoch: 161, Best Loss = 4.654759883880615\n",
            "epoch: 162, Best Loss = 4.654759883880615\n",
            "epoch: 163, Best Loss = 4.654759883880615\n",
            "epoch: 164, Best Loss = 4.654759883880615\n",
            "epoch: 165, Best Loss = 4.65426778793335\n",
            "epoch: 166, Best Loss = 4.65426778793335\n",
            "epoch: 167, Best Loss = 4.650644302368164\n",
            "epoch: 168, Best Loss = 4.650644302368164\n",
            "epoch: 169, Best Loss = 4.650644302368164\n",
            "epoch: 170, Best Loss = 4.650644302368164\n",
            "epoch: 171, Best Loss = 4.650644302368164\n",
            "epoch: 172, Best Loss = 4.650644302368164\n",
            "epoch: 173, Best Loss = 4.63694429397583\n",
            "epoch: 174, Best Loss = 4.63694429397583\n",
            "epoch: 175, Best Loss = 4.63694429397583\n",
            "epoch: 176, Best Loss = 4.63694429397583\n",
            "epoch: 177, Best Loss = 4.63694429397583\n",
            "epoch: 178, Best Loss = 4.633856296539307\n",
            "epoch: 179, Best Loss = 4.633856296539307\n",
            "epoch: 180, Best Loss = 4.631834030151367\n",
            "epoch: 181, Best Loss = 4.631834030151367\n",
            "epoch: 182, Best Loss = 4.631834030151367\n",
            "epoch: 183, Best Loss = 4.631834030151367\n",
            "epoch: 184, Best Loss = 4.609035968780518\n",
            "epoch: 185, Best Loss = 4.609035968780518\n",
            "epoch: 186, Best Loss = 4.609035968780518\n",
            "epoch: 187, Best Loss = 4.609035968780518\n",
            "epoch: 188, Best Loss = 4.609035968780518\n",
            "epoch: 189, Best Loss = 4.609035968780518\n",
            "epoch: 190, Best Loss = 4.609035968780518\n",
            "epoch: 191, Best Loss = 4.609035968780518\n",
            "epoch: 192, Best Loss = 4.609035968780518\n",
            "epoch: 193, Best Loss = 4.609035968780518\n",
            "epoch: 194, Best Loss = 4.609035968780518\n",
            "epoch: 195, Best Loss = 4.609035968780518\n",
            "epoch: 196, Best Loss = 4.609035968780518\n",
            "epoch: 197, Best Loss = 4.609035968780518\n",
            "epoch: 198, Best Loss = 4.609035968780518\n",
            "epoch: 199, Best Loss = 4.609035968780518\n",
            "epoch: 200, Best Loss = 4.6001763343811035\n",
            "epoch: 201, Best Loss = 4.6001763343811035\n",
            "epoch: 202, Best Loss = 4.6001763343811035\n",
            "epoch: 203, Best Loss = 4.6001763343811035\n",
            "epoch: 204, Best Loss = 4.6001763343811035\n",
            "epoch: 205, Best Loss = 4.6001763343811035\n",
            "epoch: 206, Best Loss = 4.6001763343811035\n",
            "epoch: 207, Best Loss = 4.594714164733887\n",
            "epoch: 208, Best Loss = 4.590261459350586\n",
            "epoch: 209, Best Loss = 4.590261459350586\n",
            "epoch: 210, Best Loss = 4.590261459350586\n",
            "epoch: 211, Best Loss = 4.590261459350586\n",
            "epoch: 212, Best Loss = 4.590261459350586\n",
            "epoch: 213, Best Loss = 4.590261459350586\n",
            "epoch: 214, Best Loss = 4.590261459350586\n",
            "epoch: 215, Best Loss = 4.590261459350586\n",
            "epoch: 216, Best Loss = 4.590261459350586\n",
            "epoch: 217, Best Loss = 4.590261459350586\n",
            "epoch: 218, Best Loss = 4.582377910614014\n",
            "epoch: 219, Best Loss = 4.582377910614014\n",
            "epoch: 220, Best Loss = 4.582377910614014\n",
            "epoch: 221, Best Loss = 4.582377910614014\n",
            "epoch: 222, Best Loss = 4.582377910614014\n",
            "epoch: 223, Best Loss = 4.582377910614014\n",
            "epoch: 224, Best Loss = 4.582377910614014\n",
            "epoch: 225, Best Loss = 4.582377910614014\n",
            "epoch: 226, Best Loss = 4.582377910614014\n",
            "epoch: 227, Best Loss = 4.5743536949157715\n",
            "epoch: 228, Best Loss = 4.5743536949157715\n",
            "epoch: 229, Best Loss = 4.5743536949157715\n",
            "epoch: 230, Best Loss = 4.5743536949157715\n",
            "epoch: 231, Best Loss = 4.565751075744629\n",
            "epoch: 232, Best Loss = 4.565751075744629\n",
            "epoch: 233, Best Loss = 4.565751075744629\n",
            "epoch: 234, Best Loss = 4.565751075744629\n",
            "epoch: 235, Best Loss = 4.565751075744629\n",
            "epoch: 236, Best Loss = 4.558863639831543\n",
            "epoch: 237, Best Loss = 4.558406829833984\n",
            "epoch: 238, Best Loss = 4.558406829833984\n",
            "epoch: 239, Best Loss = 4.557569980621338\n",
            "epoch: 240, Best Loss = 4.557569980621338\n",
            "epoch: 241, Best Loss = 4.557569980621338\n",
            "epoch: 242, Best Loss = 4.557569980621338\n",
            "epoch: 243, Best Loss = 4.530228137969971\n",
            "epoch: 244, Best Loss = 4.530228137969971\n",
            "epoch: 245, Best Loss = 4.530228137969971\n",
            "epoch: 246, Best Loss = 4.526220798492432\n",
            "epoch: 247, Best Loss = 4.526220798492432\n",
            "epoch: 248, Best Loss = 4.526220798492432\n",
            "epoch: 249, Best Loss = 4.511466026306152\n",
            "epoch: 250, Best Loss = 4.511466026306152\n",
            "epoch: 251, Best Loss = 4.511466026306152\n",
            "epoch: 252, Best Loss = 4.511466026306152\n",
            "epoch: 253, Best Loss = 4.511466026306152\n",
            "epoch: 254, Best Loss = 4.511466026306152\n",
            "epoch: 255, Best Loss = 4.511466026306152\n",
            "epoch: 256, Best Loss = 4.511466026306152\n",
            "epoch: 257, Best Loss = 4.511466026306152\n",
            "epoch: 258, Best Loss = 4.511466026306152\n",
            "epoch: 259, Best Loss = 4.511466026306152\n",
            "epoch: 260, Best Loss = 4.511466026306152\n",
            "epoch: 261, Best Loss = 4.511466026306152\n",
            "epoch: 262, Best Loss = 4.511466026306152\n",
            "epoch: 263, Best Loss = 4.511466026306152\n",
            "epoch: 264, Best Loss = 4.511466026306152\n",
            "epoch: 265, Best Loss = 4.511466026306152\n",
            "epoch: 266, Best Loss = 4.511466026306152\n",
            "epoch: 267, Best Loss = 4.511466026306152\n",
            "epoch: 268, Best Loss = 4.511466026306152\n",
            "epoch: 269, Best Loss = 4.511466026306152\n",
            "epoch: 270, Best Loss = 4.511466026306152\n",
            "epoch: 271, Best Loss = 4.511466026306152\n",
            "epoch: 272, Best Loss = 4.511466026306152\n",
            "epoch: 273, Best Loss = 4.511466026306152\n",
            "epoch: 274, Best Loss = 4.511466026306152\n",
            "epoch: 275, Best Loss = 4.511466026306152\n",
            "epoch: 276, Best Loss = 4.511466026306152\n",
            "epoch: 277, Best Loss = 4.511466026306152\n",
            "epoch: 278, Best Loss = 4.5094170570373535\n",
            "epoch: 279, Best Loss = 4.5094170570373535\n",
            "epoch: 280, Best Loss = 4.5094170570373535\n",
            "epoch: 281, Best Loss = 4.5094170570373535\n",
            "epoch: 282, Best Loss = 4.5094170570373535\n",
            "epoch: 283, Best Loss = 4.5094170570373535\n",
            "epoch: 284, Best Loss = 4.5094170570373535\n",
            "epoch: 285, Best Loss = 4.5094170570373535\n",
            "epoch: 286, Best Loss = 4.5094170570373535\n",
            "epoch: 287, Best Loss = 4.5094170570373535\n",
            "epoch: 288, Best Loss = 4.5094170570373535\n",
            "epoch: 289, Best Loss = 4.5094170570373535\n",
            "epoch: 290, Best Loss = 4.5094170570373535\n",
            "epoch: 291, Best Loss = 4.498464107513428\n",
            "epoch: 292, Best Loss = 4.498464107513428\n",
            "epoch: 293, Best Loss = 4.498464107513428\n",
            "epoch: 294, Best Loss = 4.498464107513428\n",
            "epoch: 295, Best Loss = 4.498464107513428\n",
            "epoch: 296, Best Loss = 4.498464107513428\n",
            "epoch: 297, Best Loss = 4.498464107513428\n",
            "epoch: 298, Best Loss = 4.498464107513428\n",
            "epoch: 299, Best Loss = 4.498464107513428\n",
            "epoch: 300, Best Loss = 4.498464107513428\n",
            "epoch: 301, Best Loss = 4.498464107513428\n",
            "epoch: 302, Best Loss = 4.490377902984619\n",
            "epoch: 303, Best Loss = 4.490377902984619\n",
            "epoch: 304, Best Loss = 4.490377902984619\n",
            "epoch: 305, Best Loss = 4.490377902984619\n",
            "epoch: 306, Best Loss = 4.490377902984619\n",
            "epoch: 307, Best Loss = 4.490377902984619\n",
            "epoch: 308, Best Loss = 4.490377902984619\n",
            "epoch: 309, Best Loss = 4.490377902984619\n",
            "epoch: 310, Best Loss = 4.490377902984619\n",
            "epoch: 311, Best Loss = 4.490377902984619\n",
            "epoch: 312, Best Loss = 4.490377902984619\n",
            "epoch: 313, Best Loss = 4.489943504333496\n",
            "epoch: 314, Best Loss = 4.489943504333496\n",
            "epoch: 315, Best Loss = 4.489943504333496\n",
            "epoch: 316, Best Loss = 4.489943504333496\n",
            "epoch: 317, Best Loss = 4.489943504333496\n",
            "epoch: 318, Best Loss = 4.489943504333496\n",
            "epoch: 319, Best Loss = 4.489943504333496\n",
            "epoch: 320, Best Loss = 4.489943504333496\n",
            "epoch: 321, Best Loss = 4.489943504333496\n",
            "epoch: 322, Best Loss = 4.489943504333496\n",
            "epoch: 323, Best Loss = 4.489943504333496\n",
            "epoch: 324, Best Loss = 4.489943504333496\n",
            "epoch: 325, Best Loss = 4.489943504333496\n",
            "epoch: 326, Best Loss = 4.489943504333496\n",
            "epoch: 327, Best Loss = 4.489943504333496\n",
            "epoch: 328, Best Loss = 4.489943504333496\n",
            "epoch: 329, Best Loss = 4.489943504333496\n",
            "epoch: 330, Best Loss = 4.489943504333496\n",
            "epoch: 331, Best Loss = 4.489943504333496\n",
            "epoch: 332, Best Loss = 4.489943504333496\n",
            "epoch: 333, Best Loss = 4.489943504333496\n",
            "epoch: 334, Best Loss = 4.489943504333496\n",
            "epoch: 335, Best Loss = 4.489943504333496\n",
            "epoch: 336, Best Loss = 4.489943504333496\n",
            "epoch: 337, Best Loss = 4.489943504333496\n",
            "epoch: 338, Best Loss = 4.489943504333496\n",
            "epoch: 339, Best Loss = 4.489943504333496\n",
            "epoch: 340, Best Loss = 4.489943504333496\n",
            "epoch: 341, Best Loss = 4.489943504333496\n",
            "epoch: 342, Best Loss = 4.489943504333496\n",
            "epoch: 343, Best Loss = 4.489943504333496\n",
            "epoch: 344, Best Loss = 4.480755805969238\n",
            "epoch: 345, Best Loss = 4.479145050048828\n",
            "epoch: 346, Best Loss = 4.479145050048828\n",
            "epoch: 347, Best Loss = 4.479145050048828\n",
            "epoch: 348, Best Loss = 4.479145050048828\n",
            "epoch: 349, Best Loss = 4.479145050048828\n",
            "epoch: 350, Best Loss = 4.479145050048828\n",
            "epoch: 351, Best Loss = 4.479145050048828\n",
            "epoch: 352, Best Loss = 4.479145050048828\n",
            "epoch: 353, Best Loss = 4.479145050048828\n",
            "epoch: 354, Best Loss = 4.479145050048828\n",
            "epoch: 355, Best Loss = 4.479145050048828\n",
            "epoch: 356, Best Loss = 4.479145050048828\n",
            "epoch: 357, Best Loss = 4.479145050048828\n",
            "epoch: 358, Best Loss = 4.479145050048828\n",
            "epoch: 359, Best Loss = 4.479145050048828\n",
            "epoch: 360, Best Loss = 4.479145050048828\n",
            "epoch: 361, Best Loss = 4.479145050048828\n",
            "epoch: 362, Best Loss = 4.479145050048828\n",
            "epoch: 363, Best Loss = 4.479145050048828\n",
            "epoch: 364, Best Loss = 4.479145050048828\n",
            "epoch: 365, Best Loss = 4.479145050048828\n",
            "epoch: 366, Best Loss = 4.469007968902588\n",
            "epoch: 367, Best Loss = 4.469007968902588\n",
            "epoch: 368, Best Loss = 4.469007968902588\n",
            "epoch: 369, Best Loss = 4.469007968902588\n",
            "epoch: 370, Best Loss = 4.469007968902588\n",
            "epoch: 371, Best Loss = 4.469007968902588\n",
            "epoch: 372, Best Loss = 4.469007968902588\n",
            "epoch: 373, Best Loss = 4.469007968902588\n",
            "epoch: 374, Best Loss = 4.469007968902588\n",
            "epoch: 375, Best Loss = 4.415070533752441\n",
            "epoch: 376, Best Loss = 4.415070533752441\n",
            "epoch: 377, Best Loss = 4.415070533752441\n",
            "epoch: 378, Best Loss = 4.415070533752441\n",
            "epoch: 379, Best Loss = 4.415070533752441\n",
            "epoch: 380, Best Loss = 4.415070533752441\n",
            "epoch: 381, Best Loss = 4.415070533752441\n",
            "epoch: 382, Best Loss = 4.415070533752441\n",
            "epoch: 383, Best Loss = 4.415070533752441\n",
            "epoch: 384, Best Loss = 4.415070533752441\n",
            "epoch: 385, Best Loss = 4.415070533752441\n",
            "epoch: 386, Best Loss = 4.415070533752441\n",
            "epoch: 387, Best Loss = 4.415070533752441\n",
            "epoch: 388, Best Loss = 4.415070533752441\n",
            "epoch: 389, Best Loss = 4.415070533752441\n",
            "epoch: 390, Best Loss = 4.415070533752441\n",
            "epoch: 391, Best Loss = 4.415070533752441\n",
            "epoch: 392, Best Loss = 4.415070533752441\n",
            "epoch: 393, Best Loss = 4.415070533752441\n",
            "epoch: 394, Best Loss = 4.415070533752441\n",
            "epoch: 395, Best Loss = 4.415070533752441\n",
            "epoch: 396, Best Loss = 4.415070533752441\n",
            "epoch: 397, Best Loss = 4.415070533752441\n",
            "epoch: 398, Best Loss = 4.415070533752441\n",
            "epoch: 399, Best Loss = 4.415070533752441\n",
            "epoch: 400, Best Loss = 4.415070533752441\n",
            "epoch: 401, Best Loss = 4.415070533752441\n",
            "epoch: 402, Best Loss = 4.415070533752441\n",
            "epoch: 403, Best Loss = 4.415070533752441\n",
            "epoch: 404, Best Loss = 4.415070533752441\n",
            "epoch: 405, Best Loss = 4.415070533752441\n",
            "epoch: 406, Best Loss = 4.415070533752441\n",
            "epoch: 407, Best Loss = 4.415070533752441\n",
            "epoch: 408, Best Loss = 4.415070533752441\n",
            "epoch: 409, Best Loss = 4.415070533752441\n",
            "epoch: 410, Best Loss = 4.415070533752441\n",
            "epoch: 411, Best Loss = 4.415070533752441\n",
            "epoch: 412, Best Loss = 4.415070533752441\n",
            "epoch: 413, Best Loss = 4.415070533752441\n",
            "epoch: 414, Best Loss = 4.415070533752441\n",
            "epoch: 415, Best Loss = 4.415070533752441\n",
            "epoch: 416, Best Loss = 4.415070533752441\n",
            "epoch: 417, Best Loss = 4.415070533752441\n",
            "epoch: 418, Best Loss = 4.415070533752441\n",
            "epoch: 419, Best Loss = 4.415070533752441\n",
            "epoch: 420, Best Loss = 4.415070533752441\n",
            "epoch: 421, Best Loss = 4.415070533752441\n",
            "epoch: 422, Best Loss = 4.415070533752441\n",
            "epoch: 423, Best Loss = 4.415070533752441\n",
            "epoch: 424, Best Loss = 4.415070533752441\n",
            "epoch: 425, Best Loss = 4.415070533752441\n",
            "epoch: 426, Best Loss = 4.415070533752441\n",
            "epoch: 427, Best Loss = 4.415070533752441\n",
            "epoch: 428, Best Loss = 4.415070533752441\n",
            "epoch: 429, Best Loss = 4.415070533752441\n",
            "epoch: 430, Best Loss = 4.415070533752441\n",
            "epoch: 431, Best Loss = 4.415070533752441\n",
            "epoch: 432, Best Loss = 4.415070533752441\n",
            "epoch: 433, Best Loss = 4.415070533752441\n",
            "epoch: 434, Best Loss = 4.415070533752441\n",
            "epoch: 435, Best Loss = 4.415070533752441\n",
            "epoch: 436, Best Loss = 4.415070533752441\n",
            "epoch: 437, Best Loss = 4.415070533752441\n",
            "epoch: 438, Best Loss = 4.415070533752441\n",
            "epoch: 439, Best Loss = 4.415070533752441\n",
            "epoch: 440, Best Loss = 4.415070533752441\n",
            "epoch: 441, Best Loss = 4.415070533752441\n",
            "epoch: 442, Best Loss = 4.415070533752441\n",
            "epoch: 443, Best Loss = 4.415070533752441\n",
            "epoch: 444, Best Loss = 4.415070533752441\n",
            "epoch: 445, Best Loss = 4.415070533752441\n",
            "epoch: 446, Best Loss = 4.415070533752441\n",
            "epoch: 447, Best Loss = 4.415070533752441\n",
            "epoch: 448, Best Loss = 4.4120588302612305\n",
            "epoch: 449, Best Loss = 4.4120588302612305\n",
            "epoch: 450, Best Loss = 4.4120588302612305\n",
            "epoch: 451, Best Loss = 4.4120588302612305\n",
            "epoch: 452, Best Loss = 4.4120588302612305\n",
            "epoch: 453, Best Loss = 4.4120588302612305\n",
            "epoch: 454, Best Loss = 4.4120588302612305\n",
            "epoch: 455, Best Loss = 4.4120588302612305\n",
            "epoch: 456, Best Loss = 4.4120588302612305\n",
            "epoch: 457, Best Loss = 4.4120588302612305\n",
            "epoch: 458, Best Loss = 4.4120588302612305\n",
            "epoch: 459, Best Loss = 4.4120588302612305\n",
            "epoch: 460, Best Loss = 4.404582500457764\n",
            "epoch: 461, Best Loss = 4.397038459777832\n",
            "epoch: 462, Best Loss = 4.397038459777832\n",
            "epoch: 463, Best Loss = 4.397038459777832\n",
            "epoch: 464, Best Loss = 4.397038459777832\n",
            "epoch: 465, Best Loss = 4.397038459777832\n",
            "epoch: 466, Best Loss = 4.397038459777832\n",
            "epoch: 467, Best Loss = 4.397038459777832\n",
            "epoch: 468, Best Loss = 4.3919172286987305\n",
            "epoch: 469, Best Loss = 4.3919172286987305\n",
            "epoch: 470, Best Loss = 4.3919172286987305\n",
            "epoch: 471, Best Loss = 4.3919172286987305\n",
            "epoch: 472, Best Loss = 4.3919172286987305\n",
            "epoch: 473, Best Loss = 4.3919172286987305\n",
            "epoch: 474, Best Loss = 4.3919172286987305\n",
            "epoch: 475, Best Loss = 4.370656490325928\n",
            "epoch: 476, Best Loss = 4.370656490325928\n",
            "epoch: 477, Best Loss = 4.370656490325928\n",
            "epoch: 478, Best Loss = 4.370656490325928\n",
            "epoch: 479, Best Loss = 4.370656490325928\n",
            "epoch: 480, Best Loss = 4.362483978271484\n",
            "epoch: 481, Best Loss = 4.362483978271484\n",
            "epoch: 482, Best Loss = 4.362483978271484\n",
            "epoch: 483, Best Loss = 4.362483978271484\n",
            "epoch: 484, Best Loss = 4.362483978271484\n",
            "epoch: 485, Best Loss = 4.362483978271484\n",
            "epoch: 486, Best Loss = 4.362483978271484\n",
            "epoch: 487, Best Loss = 4.341840744018555\n",
            "epoch: 488, Best Loss = 4.341840744018555\n",
            "epoch: 489, Best Loss = 4.341840744018555\n",
            "epoch: 490, Best Loss = 4.341840744018555\n",
            "epoch: 491, Best Loss = 4.3415398597717285\n",
            "epoch: 492, Best Loss = 4.3415398597717285\n",
            "epoch: 493, Best Loss = 4.3415398597717285\n",
            "epoch: 494, Best Loss = 4.3415398597717285\n",
            "epoch: 495, Best Loss = 4.3415398597717285\n",
            "epoch: 496, Best Loss = 4.3415398597717285\n",
            "epoch: 497, Best Loss = 4.3415398597717285\n",
            "epoch: 498, Best Loss = 4.3415398597717285\n",
            "epoch: 499, Best Loss = 4.3415398597717285\n",
            "epoch: 500, Best Loss = 4.3415398597717285\n",
            "epoch: 501, Best Loss = 4.3415398597717285\n",
            "epoch: 502, Best Loss = 4.337708473205566\n",
            "epoch: 503, Best Loss = 4.337708473205566\n",
            "epoch: 504, Best Loss = 4.275686264038086\n",
            "epoch: 505, Best Loss = 4.275686264038086\n",
            "epoch: 506, Best Loss = 4.275686264038086\n",
            "epoch: 507, Best Loss = 4.275686264038086\n",
            "epoch: 508, Best Loss = 4.275686264038086\n",
            "epoch: 509, Best Loss = 4.275686264038086\n",
            "epoch: 510, Best Loss = 4.275686264038086\n",
            "epoch: 511, Best Loss = 4.275686264038086\n",
            "epoch: 512, Best Loss = 4.275686264038086\n",
            "epoch: 513, Best Loss = 4.2444281578063965\n",
            "epoch: 514, Best Loss = 4.2444281578063965\n",
            "epoch: 515, Best Loss = 4.2444281578063965\n",
            "epoch: 516, Best Loss = 4.2444281578063965\n",
            "epoch: 517, Best Loss = 4.2444281578063965\n",
            "epoch: 518, Best Loss = 4.2444281578063965\n",
            "epoch: 519, Best Loss = 4.2444281578063965\n",
            "epoch: 520, Best Loss = 4.2444281578063965\n",
            "epoch: 521, Best Loss = 4.2444281578063965\n",
            "epoch: 522, Best Loss = 4.232889652252197\n",
            "epoch: 523, Best Loss = 4.232889652252197\n",
            "epoch: 524, Best Loss = 4.232889652252197\n",
            "epoch: 525, Best Loss = 4.232889652252197\n",
            "epoch: 526, Best Loss = 4.232889652252197\n",
            "epoch: 527, Best Loss = 4.232889652252197\n",
            "epoch: 528, Best Loss = 4.232889652252197\n",
            "epoch: 529, Best Loss = 4.232889652252197\n",
            "epoch: 530, Best Loss = 4.232889652252197\n",
            "epoch: 531, Best Loss = 4.232889652252197\n",
            "epoch: 532, Best Loss = 4.232889652252197\n",
            "epoch: 533, Best Loss = 4.232889652252197\n",
            "epoch: 534, Best Loss = 4.232889652252197\n",
            "epoch: 535, Best Loss = 4.232889652252197\n",
            "epoch: 536, Best Loss = 4.232889652252197\n",
            "epoch: 537, Best Loss = 4.232889652252197\n",
            "epoch: 538, Best Loss = 4.232889652252197\n",
            "epoch: 539, Best Loss = 4.232889652252197\n",
            "epoch: 540, Best Loss = 4.20079231262207\n",
            "epoch: 541, Best Loss = 4.195517539978027\n",
            "epoch: 542, Best Loss = 4.195517539978027\n",
            "epoch: 543, Best Loss = 4.152657508850098\n",
            "epoch: 544, Best Loss = 4.122649669647217\n",
            "epoch: 545, Best Loss = 4.122649669647217\n",
            "epoch: 546, Best Loss = 4.121587753295898\n",
            "epoch: 547, Best Loss = 4.121587753295898\n",
            "epoch: 548, Best Loss = 4.121587753295898\n",
            "epoch: 549, Best Loss = 4.121587753295898\n",
            "epoch: 550, Best Loss = 4.121587753295898\n",
            "epoch: 551, Best Loss = 4.0844221115112305\n",
            "epoch: 552, Best Loss = 4.0844221115112305\n",
            "epoch: 553, Best Loss = 4.0844221115112305\n",
            "epoch: 554, Best Loss = 4.0844221115112305\n",
            "epoch: 555, Best Loss = 4.0844221115112305\n",
            "epoch: 556, Best Loss = 4.0844221115112305\n",
            "epoch: 557, Best Loss = 4.0844221115112305\n",
            "epoch: 558, Best Loss = 4.0844221115112305\n",
            "epoch: 559, Best Loss = 4.080368518829346\n",
            "epoch: 560, Best Loss = 4.033050060272217\n",
            "epoch: 561, Best Loss = 4.014368057250977\n",
            "epoch: 562, Best Loss = 4.014368057250977\n",
            "epoch: 563, Best Loss = 4.014368057250977\n",
            "epoch: 564, Best Loss = 4.014368057250977\n",
            "epoch: 565, Best Loss = 4.014368057250977\n",
            "epoch: 566, Best Loss = 4.014368057250977\n",
            "epoch: 567, Best Loss = 4.014368057250977\n",
            "epoch: 568, Best Loss = 3.9795751571655273\n",
            "epoch: 569, Best Loss = 3.9795751571655273\n",
            "epoch: 570, Best Loss = 3.9497904777526855\n",
            "epoch: 571, Best Loss = 3.9497904777526855\n",
            "epoch: 572, Best Loss = 3.9497904777526855\n",
            "epoch: 573, Best Loss = 3.936544418334961\n",
            "epoch: 574, Best Loss = 3.936544418334961\n",
            "epoch: 575, Best Loss = 3.875404119491577\n",
            "epoch: 576, Best Loss = 3.873504638671875\n",
            "epoch: 577, Best Loss = 3.873504638671875\n",
            "epoch: 578, Best Loss = 3.873504638671875\n",
            "epoch: 579, Best Loss = 3.873504638671875\n",
            "epoch: 580, Best Loss = 3.873504638671875\n",
            "epoch: 581, Best Loss = 3.873504638671875\n",
            "epoch: 582, Best Loss = 3.873504638671875\n",
            "epoch: 583, Best Loss = 3.873504638671875\n",
            "epoch: 584, Best Loss = 3.873504638671875\n",
            "epoch: 585, Best Loss = 3.873504638671875\n",
            "epoch: 586, Best Loss = 3.8396239280700684\n",
            "epoch: 587, Best Loss = 3.786648988723755\n",
            "epoch: 588, Best Loss = 3.786648988723755\n",
            "epoch: 589, Best Loss = 3.786648988723755\n",
            "epoch: 590, Best Loss = 3.786648988723755\n",
            "epoch: 591, Best Loss = 3.7720813751220703\n",
            "epoch: 592, Best Loss = 3.72808575630188\n",
            "epoch: 593, Best Loss = 3.72808575630188\n",
            "epoch: 594, Best Loss = 3.702136993408203\n",
            "epoch: 595, Best Loss = 3.6512458324432373\n",
            "epoch: 596, Best Loss = 3.6512458324432373\n",
            "epoch: 597, Best Loss = 3.6512458324432373\n",
            "epoch: 598, Best Loss = 3.6512458324432373\n",
            "epoch: 599, Best Loss = 3.6512458324432373\n",
            "epoch: 600, Best Loss = 3.6512458324432373\n",
            "epoch: 601, Best Loss = 3.6512458324432373\n",
            "epoch: 602, Best Loss = 3.6512458324432373\n",
            "epoch: 603, Best Loss = 3.6512458324432373\n",
            "epoch: 604, Best Loss = 3.6115055084228516\n",
            "epoch: 605, Best Loss = 3.6075375080108643\n",
            "epoch: 606, Best Loss = 3.594090461730957\n",
            "epoch: 607, Best Loss = 3.535252571105957\n",
            "epoch: 608, Best Loss = 3.496387481689453\n",
            "epoch: 609, Best Loss = 3.496387481689453\n",
            "epoch: 610, Best Loss = 3.489902973175049\n",
            "epoch: 611, Best Loss = 3.489902973175049\n",
            "epoch: 612, Best Loss = 3.4255852699279785\n",
            "epoch: 613, Best Loss = 3.4255852699279785\n",
            "epoch: 614, Best Loss = 3.4255852699279785\n",
            "epoch: 615, Best Loss = 3.4099056720733643\n",
            "epoch: 616, Best Loss = 3.4099056720733643\n",
            "epoch: 617, Best Loss = 3.375983238220215\n",
            "epoch: 618, Best Loss = 3.3270986080169678\n",
            "epoch: 619, Best Loss = 3.3270986080169678\n",
            "epoch: 620, Best Loss = 3.3270986080169678\n",
            "epoch: 621, Best Loss = 3.3270986080169678\n",
            "epoch: 622, Best Loss = 3.3270986080169678\n",
            "epoch: 623, Best Loss = 3.3270986080169678\n",
            "epoch: 624, Best Loss = 3.3270986080169678\n",
            "epoch: 625, Best Loss = 3.3270986080169678\n",
            "epoch: 626, Best Loss = 3.3270986080169678\n",
            "epoch: 627, Best Loss = 3.3270986080169678\n",
            "epoch: 628, Best Loss = 3.2273824214935303\n",
            "epoch: 629, Best Loss = 3.2025890350341797\n",
            "epoch: 630, Best Loss = 3.146885871887207\n",
            "epoch: 631, Best Loss = 3.146885871887207\n",
            "epoch: 632, Best Loss = 3.146885871887207\n",
            "epoch: 633, Best Loss = 3.146885871887207\n",
            "epoch: 634, Best Loss = 3.078245162963867\n",
            "epoch: 635, Best Loss = 3.078245162963867\n",
            "epoch: 636, Best Loss = 3.078245162963867\n",
            "epoch: 637, Best Loss = 3.027656316757202\n",
            "epoch: 638, Best Loss = 3.027656316757202\n",
            "epoch: 639, Best Loss = 3.027656316757202\n",
            "epoch: 640, Best Loss = 2.8816518783569336\n",
            "epoch: 641, Best Loss = 2.8816518783569336\n",
            "epoch: 642, Best Loss = 2.8816518783569336\n",
            "epoch: 643, Best Loss = 2.8816518783569336\n",
            "epoch: 644, Best Loss = 2.8816518783569336\n",
            "epoch: 645, Best Loss = 2.8816518783569336\n",
            "epoch: 646, Best Loss = 2.8816518783569336\n",
            "epoch: 647, Best Loss = 2.8816518783569336\n",
            "epoch: 648, Best Loss = 2.864346742630005\n",
            "epoch: 649, Best Loss = 2.8434898853302\n",
            "epoch: 650, Best Loss = 2.785597324371338\n",
            "epoch: 651, Best Loss = 2.785597324371338\n",
            "epoch: 652, Best Loss = 2.785597324371338\n",
            "epoch: 653, Best Loss = 2.776432991027832\n",
            "epoch: 654, Best Loss = 2.776432991027832\n",
            "epoch: 655, Best Loss = 2.776432991027832\n",
            "epoch: 656, Best Loss = 2.6543846130371094\n",
            "epoch: 657, Best Loss = 2.6543846130371094\n",
            "epoch: 658, Best Loss = 2.6543846130371094\n",
            "epoch: 659, Best Loss = 2.6543846130371094\n",
            "epoch: 660, Best Loss = 2.587946891784668\n",
            "epoch: 661, Best Loss = 2.587946891784668\n",
            "epoch: 662, Best Loss = 2.587946891784668\n",
            "epoch: 663, Best Loss = 2.5650951862335205\n",
            "epoch: 664, Best Loss = 2.523766040802002\n",
            "epoch: 665, Best Loss = 2.523766040802002\n",
            "epoch: 666, Best Loss = 2.523766040802002\n",
            "epoch: 667, Best Loss = 2.523766040802002\n",
            "epoch: 668, Best Loss = 2.523766040802002\n",
            "epoch: 669, Best Loss = 2.523766040802002\n",
            "epoch: 670, Best Loss = 2.3884501457214355\n",
            "epoch: 671, Best Loss = 2.354058027267456\n",
            "epoch: 672, Best Loss = 2.246678352355957\n",
            "epoch: 673, Best Loss = 2.246678352355957\n",
            "epoch: 674, Best Loss = 2.246678352355957\n",
            "epoch: 675, Best Loss = 2.232978343963623\n",
            "epoch: 676, Best Loss = 2.2224957942962646\n",
            "epoch: 677, Best Loss = 2.1819963455200195\n",
            "epoch: 678, Best Loss = 2.0294675827026367\n",
            "epoch: 679, Best Loss = 2.0294675827026367\n",
            "epoch: 680, Best Loss = 1.952794075012207\n",
            "epoch: 681, Best Loss = 1.952794075012207\n",
            "epoch: 682, Best Loss = 1.952794075012207\n",
            "epoch: 683, Best Loss = 1.8782575130462646\n",
            "epoch: 684, Best Loss = 1.738969326019287\n",
            "epoch: 685, Best Loss = 1.711169958114624\n",
            "epoch: 686, Best Loss = 1.711169958114624\n",
            "epoch: 687, Best Loss = 1.6067390441894531\n",
            "epoch: 688, Best Loss = 1.6067390441894531\n",
            "epoch: 689, Best Loss = 1.499021053314209\n",
            "epoch: 690, Best Loss = 1.499021053314209\n",
            "epoch: 691, Best Loss = 1.3994708061218262\n",
            "epoch: 692, Best Loss = 1.3450517654418945\n",
            "epoch: 693, Best Loss = 1.2473700046539307\n",
            "epoch: 694, Best Loss = 1.2171690464019775\n",
            "epoch: 695, Best Loss = 1.0991387367248535\n",
            "epoch: 696, Best Loss = 1.0735907554626465\n",
            "epoch: 697, Best Loss = 1.032300591468811\n",
            "epoch: 698, Best Loss = 0.9615962505340576\n",
            "epoch: 699, Best Loss = 0.9615962505340576\n",
            "epoch: 700, Best Loss = 0.8064881563186646\n",
            "epoch: 701, Best Loss = 0.7851481437683105\n",
            "epoch: 702, Best Loss = 0.6986318826675415\n",
            "epoch: 703, Best Loss = 0.6704645752906799\n",
            "epoch: 704, Best Loss = 0.6704645752906799\n",
            "epoch: 705, Best Loss = 0.6452273726463318\n",
            "epoch: 706, Best Loss = 0.5078785419464111\n",
            "epoch: 707, Best Loss = 0.5078785419464111\n",
            "epoch: 708, Best Loss = 0.48427683115005493\n",
            "epoch: 709, Best Loss = 0.44510072469711304\n",
            "epoch: 710, Best Loss = 0.4142249822616577\n",
            "epoch: 711, Best Loss = 0.4142249822616577\n",
            "epoch: 712, Best Loss = 0.3007708787918091\n",
            "epoch: 713, Best Loss = 0.3007708787918091\n",
            "epoch: 714, Best Loss = 0.2686164081096649\n",
            "epoch: 715, Best Loss = 0.23241674900054932\n",
            "epoch: 716, Best Loss = 0.20218947529792786\n",
            "epoch: 717, Best Loss = 0.20218947529792786\n",
            "epoch: 718, Best Loss = 0.170800119638443\n",
            "epoch: 719, Best Loss = 0.16711199283599854\n",
            "epoch: 720, Best Loss = 0.16711199283599854\n",
            "epoch: 721, Best Loss = 0.14157867431640625\n",
            "epoch: 722, Best Loss = 0.14157867431640625\n",
            "epoch: 723, Best Loss = 0.13886353373527527\n",
            "epoch: 724, Best Loss = 0.13886353373527527\n",
            "epoch: 725, Best Loss = 0.12362651526927948\n",
            "epoch: 726, Best Loss = 0.12362651526927948\n",
            "epoch: 727, Best Loss = 0.12362651526927948\n",
            "epoch: 728, Best Loss = 0.12362651526927948\n",
            "epoch: 729, Best Loss = 0.12362651526927948\n",
            "epoch: 730, Best Loss = 0.12362651526927948\n",
            "epoch: 731, Best Loss = 0.12362651526927948\n",
            "epoch: 732, Best Loss = 0.12362651526927948\n",
            "epoch: 733, Best Loss = 0.09309840202331543\n",
            "epoch: 734, Best Loss = 0.09309840202331543\n",
            "epoch: 735, Best Loss = 0.0924435704946518\n",
            "epoch: 736, Best Loss = 0.0833926722407341\n",
            "epoch: 737, Best Loss = 0.08221613615751266\n",
            "epoch: 738, Best Loss = 0.08221613615751266\n",
            "epoch: 739, Best Loss = 0.08221613615751266\n",
            "epoch: 740, Best Loss = 0.08221613615751266\n",
            "epoch: 741, Best Loss = 0.07457844913005829\n",
            "epoch: 742, Best Loss = 0.07191521674394608\n",
            "epoch: 743, Best Loss = 0.0677604228258133\n",
            "epoch: 744, Best Loss = 0.0677604228258133\n",
            "epoch: 745, Best Loss = 0.0677604228258133\n",
            "epoch: 746, Best Loss = 0.0677604228258133\n",
            "epoch: 747, Best Loss = 0.0677604228258133\n",
            "epoch: 748, Best Loss = 0.0677604228258133\n",
            "epoch: 749, Best Loss = 0.0677604228258133\n",
            "epoch: 750, Best Loss = 0.0677604228258133\n",
            "epoch: 751, Best Loss = 0.0677604228258133\n",
            "epoch: 752, Best Loss = 0.0677604228258133\n",
            "epoch: 753, Best Loss = 0.0677604228258133\n",
            "epoch: 754, Best Loss = 0.0677604228258133\n",
            "epoch: 755, Best Loss = 0.0677604228258133\n",
            "epoch: 756, Best Loss = 0.0677604228258133\n",
            "epoch: 757, Best Loss = 0.0677604228258133\n",
            "epoch: 758, Best Loss = 0.0677604228258133\n",
            "epoch: 759, Best Loss = 0.0677604228258133\n",
            "epoch: 760, Best Loss = 0.0677604228258133\n",
            "epoch: 761, Best Loss = 0.06595593690872192\n",
            "epoch: 762, Best Loss = 0.06595593690872192\n",
            "epoch: 763, Best Loss = 0.06595593690872192\n",
            "epoch: 764, Best Loss = 0.06595593690872192\n",
            "epoch: 765, Best Loss = 0.0625203475356102\n",
            "epoch: 766, Best Loss = 0.0625203475356102\n",
            "epoch: 767, Best Loss = 0.04917571693658829\n",
            "epoch: 768, Best Loss = 0.04917571693658829\n",
            "epoch: 769, Best Loss = 0.04917571693658829\n",
            "epoch: 770, Best Loss = 0.04917571693658829\n",
            "epoch: 771, Best Loss = 0.04917571693658829\n",
            "epoch: 772, Best Loss = 0.04917571693658829\n",
            "epoch: 773, Best Loss = 0.04917571693658829\n",
            "epoch: 774, Best Loss = 0.04917571693658829\n",
            "epoch: 775, Best Loss = 0.04917571693658829\n",
            "epoch: 776, Best Loss = 0.04917571693658829\n",
            "epoch: 777, Best Loss = 0.04917571693658829\n",
            "epoch: 778, Best Loss = 0.04917571693658829\n",
            "epoch: 779, Best Loss = 0.04917571693658829\n",
            "epoch: 780, Best Loss = 0.04917571693658829\n",
            "epoch: 781, Best Loss = 0.04917571693658829\n",
            "epoch: 782, Best Loss = 0.04917571693658829\n",
            "epoch: 783, Best Loss = 0.040912266820669174\n",
            "epoch: 784, Best Loss = 0.040912266820669174\n",
            "epoch: 785, Best Loss = 0.040912266820669174\n",
            "epoch: 786, Best Loss = 0.040912266820669174\n",
            "epoch: 787, Best Loss = 0.040912266820669174\n",
            "epoch: 788, Best Loss = 0.040912266820669174\n",
            "epoch: 789, Best Loss = 0.0347837470471859\n",
            "epoch: 790, Best Loss = 0.0347837470471859\n",
            "epoch: 791, Best Loss = 0.0347837470471859\n",
            "epoch: 792, Best Loss = 0.0347837470471859\n",
            "epoch: 793, Best Loss = 0.0347837470471859\n",
            "epoch: 794, Best Loss = 0.0347837470471859\n",
            "epoch: 795, Best Loss = 0.0347837470471859\n",
            "epoch: 796, Best Loss = 0.0347837470471859\n",
            "epoch: 797, Best Loss = 0.0347837470471859\n",
            "epoch: 798, Best Loss = 0.0347837470471859\n",
            "epoch: 799, Best Loss = 0.0347837470471859\n",
            "epoch: 800, Best Loss = 0.034320585429668427\n",
            "epoch: 801, Best Loss = 0.034320585429668427\n",
            "epoch: 802, Best Loss = 0.034320585429668427\n",
            "epoch: 803, Best Loss = 0.034320585429668427\n",
            "epoch: 804, Best Loss = 0.03257596492767334\n",
            "epoch: 805, Best Loss = 0.03257596492767334\n",
            "epoch: 806, Best Loss = 0.03257596492767334\n",
            "epoch: 807, Best Loss = 0.03257596492767334\n",
            "epoch: 808, Best Loss = 0.03257596492767334\n",
            "epoch: 809, Best Loss = 0.03257596492767334\n",
            "epoch: 810, Best Loss = 0.029598267748951912\n",
            "epoch: 811, Best Loss = 0.029598267748951912\n",
            "epoch: 812, Best Loss = 0.029598267748951912\n",
            "epoch: 813, Best Loss = 0.029598267748951912\n",
            "epoch: 814, Best Loss = 0.029598267748951912\n",
            "epoch: 815, Best Loss = 0.029598267748951912\n",
            "epoch: 816, Best Loss = 0.029598267748951912\n",
            "epoch: 817, Best Loss = 0.029598267748951912\n",
            "epoch: 818, Best Loss = 0.029598267748951912\n",
            "epoch: 819, Best Loss = 0.029598267748951912\n",
            "epoch: 820, Best Loss = 0.029598267748951912\n",
            "epoch: 821, Best Loss = 0.029598267748951912\n",
            "epoch: 822, Best Loss = 0.029598267748951912\n",
            "epoch: 823, Best Loss = 0.029598267748951912\n",
            "epoch: 824, Best Loss = 0.02902522124350071\n",
            "epoch: 825, Best Loss = 0.02902522124350071\n",
            "epoch: 826, Best Loss = 0.02902522124350071\n",
            "epoch: 827, Best Loss = 0.02902522124350071\n",
            "epoch: 828, Best Loss = 0.02902522124350071\n",
            "epoch: 829, Best Loss = 0.02902522124350071\n",
            "epoch: 830, Best Loss = 0.026952119544148445\n",
            "epoch: 831, Best Loss = 0.026952119544148445\n",
            "epoch: 832, Best Loss = 0.026952119544148445\n",
            "epoch: 833, Best Loss = 0.026952119544148445\n",
            "epoch: 834, Best Loss = 0.025431279093027115\n",
            "epoch: 835, Best Loss = 0.025431279093027115\n",
            "epoch: 836, Best Loss = 0.025431279093027115\n",
            "epoch: 837, Best Loss = 0.025431279093027115\n",
            "epoch: 838, Best Loss = 0.025431279093027115\n",
            "epoch: 839, Best Loss = 0.025431279093027115\n",
            "epoch: 840, Best Loss = 0.025431279093027115\n",
            "epoch: 841, Best Loss = 0.025431279093027115\n",
            "epoch: 842, Best Loss = 0.025431279093027115\n",
            "epoch: 843, Best Loss = 0.025431279093027115\n",
            "epoch: 844, Best Loss = 0.025431279093027115\n",
            "epoch: 845, Best Loss = 0.025431279093027115\n",
            "epoch: 846, Best Loss = 0.025431279093027115\n",
            "epoch: 847, Best Loss = 0.02449282445013523\n",
            "epoch: 848, Best Loss = 0.02449282445013523\n",
            "epoch: 849, Best Loss = 0.02449282445013523\n",
            "epoch: 850, Best Loss = 0.02449282445013523\n",
            "epoch: 851, Best Loss = 0.02449282445013523\n",
            "epoch: 852, Best Loss = 0.02449282445013523\n",
            "epoch: 853, Best Loss = 0.02449282445013523\n",
            "epoch: 854, Best Loss = 0.02449282445013523\n",
            "epoch: 855, Best Loss = 0.02449282445013523\n",
            "epoch: 856, Best Loss = 0.02449282445013523\n",
            "epoch: 857, Best Loss = 0.023590469732880592\n",
            "epoch: 858, Best Loss = 0.02240169420838356\n",
            "epoch: 859, Best Loss = 0.02240169420838356\n",
            "epoch: 860, Best Loss = 0.02177828922867775\n",
            "epoch: 861, Best Loss = 0.02166365273296833\n",
            "epoch: 862, Best Loss = 0.02166365273296833\n",
            "epoch: 863, Best Loss = 0.02166365273296833\n",
            "epoch: 864, Best Loss = 0.02166365273296833\n",
            "epoch: 865, Best Loss = 0.02166365273296833\n",
            "epoch: 866, Best Loss = 0.02166365273296833\n",
            "epoch: 867, Best Loss = 0.02166365273296833\n",
            "epoch: 868, Best Loss = 0.02166365273296833\n",
            "epoch: 869, Best Loss = 0.02166365273296833\n",
            "epoch: 870, Best Loss = 0.02166365273296833\n",
            "epoch: 871, Best Loss = 0.02166365273296833\n",
            "epoch: 872, Best Loss = 0.02166365273296833\n",
            "epoch: 873, Best Loss = 0.02166365273296833\n",
            "epoch: 874, Best Loss = 0.02166365273296833\n",
            "epoch: 875, Best Loss = 0.02166365273296833\n",
            "epoch: 876, Best Loss = 0.02166365273296833\n",
            "epoch: 877, Best Loss = 0.02166365273296833\n",
            "epoch: 878, Best Loss = 0.02166365273296833\n",
            "epoch: 879, Best Loss = 0.02166365273296833\n",
            "epoch: 880, Best Loss = 0.02166365273296833\n",
            "epoch: 881, Best Loss = 0.02166365273296833\n",
            "epoch: 882, Best Loss = 0.02166365273296833\n",
            "epoch: 883, Best Loss = 0.02166365273296833\n",
            "epoch: 884, Best Loss = 0.02166365273296833\n",
            "epoch: 885, Best Loss = 0.02166365273296833\n",
            "epoch: 886, Best Loss = 0.02166365273296833\n",
            "epoch: 887, Best Loss = 0.02166365273296833\n",
            "epoch: 888, Best Loss = 0.02166365273296833\n",
            "epoch: 889, Best Loss = 0.02166365273296833\n",
            "epoch: 890, Best Loss = 0.02166365273296833\n",
            "epoch: 891, Best Loss = 0.02166365273296833\n",
            "epoch: 892, Best Loss = 0.02166365273296833\n",
            "epoch: 893, Best Loss = 0.02166365273296833\n",
            "epoch: 894, Best Loss = 0.02166365273296833\n",
            "epoch: 895, Best Loss = 0.02166365273296833\n",
            "epoch: 896, Best Loss = 0.02166365273296833\n",
            "epoch: 897, Best Loss = 0.02166365273296833\n",
            "epoch: 898, Best Loss = 0.02077074535191059\n",
            "epoch: 899, Best Loss = 0.02077074535191059\n",
            "epoch: 900, Best Loss = 0.02077074535191059\n",
            "epoch: 901, Best Loss = 0.02077074535191059\n",
            "epoch: 902, Best Loss = 0.02077074535191059\n",
            "epoch: 903, Best Loss = 0.02077074535191059\n",
            "epoch: 904, Best Loss = 0.02077074535191059\n",
            "epoch: 905, Best Loss = 0.02077074535191059\n",
            "epoch: 906, Best Loss = 0.02077074535191059\n",
            "epoch: 907, Best Loss = 0.02077074535191059\n",
            "epoch: 908, Best Loss = 0.018649009987711906\n",
            "epoch: 909, Best Loss = 0.018649009987711906\n",
            "epoch: 910, Best Loss = 0.018649009987711906\n",
            "epoch: 911, Best Loss = 0.018649009987711906\n",
            "epoch: 912, Best Loss = 0.018649009987711906\n",
            "epoch: 913, Best Loss = 0.018649009987711906\n",
            "epoch: 914, Best Loss = 0.018649009987711906\n",
            "epoch: 915, Best Loss = 0.018649009987711906\n",
            "epoch: 916, Best Loss = 0.01801822893321514\n",
            "epoch: 917, Best Loss = 0.01801822893321514\n",
            "epoch: 918, Best Loss = 0.01801822893321514\n",
            "epoch: 919, Best Loss = 0.01801822893321514\n",
            "epoch: 920, Best Loss = 0.01801822893321514\n",
            "epoch: 921, Best Loss = 0.01801822893321514\n",
            "epoch: 922, Best Loss = 0.01801822893321514\n",
            "epoch: 923, Best Loss = 0.01801822893321514\n",
            "epoch: 924, Best Loss = 0.01801822893321514\n",
            "epoch: 925, Best Loss = 0.01801822893321514\n",
            "epoch: 926, Best Loss = 0.01801822893321514\n",
            "epoch: 927, Best Loss = 0.01801822893321514\n",
            "epoch: 928, Best Loss = 0.01801822893321514\n",
            "epoch: 929, Best Loss = 0.01801822893321514\n",
            "epoch: 930, Best Loss = 0.01801822893321514\n",
            "epoch: 931, Best Loss = 0.01801822893321514\n",
            "epoch: 932, Best Loss = 0.01801822893321514\n",
            "epoch: 933, Best Loss = 0.01801822893321514\n",
            "epoch: 934, Best Loss = 0.01801822893321514\n",
            "epoch: 935, Best Loss = 0.01801822893321514\n",
            "epoch: 936, Best Loss = 0.01801822893321514\n",
            "epoch: 937, Best Loss = 0.01801822893321514\n",
            "epoch: 938, Best Loss = 0.01801822893321514\n",
            "epoch: 939, Best Loss = 0.01801822893321514\n",
            "epoch: 940, Best Loss = 0.01801822893321514\n",
            "epoch: 941, Best Loss = 0.017761852592229843\n",
            "epoch: 942, Best Loss = 0.017761852592229843\n",
            "epoch: 943, Best Loss = 0.017761852592229843\n",
            "epoch: 944, Best Loss = 0.017761852592229843\n",
            "epoch: 945, Best Loss = 0.01688632369041443\n",
            "epoch: 946, Best Loss = 0.01688632369041443\n",
            "epoch: 947, Best Loss = 0.01688632369041443\n",
            "epoch: 948, Best Loss = 0.01688632369041443\n",
            "epoch: 949, Best Loss = 0.01688632369041443\n",
            "epoch: 950, Best Loss = 0.01688632369041443\n",
            "epoch: 951, Best Loss = 0.01688632369041443\n",
            "epoch: 952, Best Loss = 0.01688632369041443\n",
            "epoch: 953, Best Loss = 0.01688632369041443\n",
            "epoch: 954, Best Loss = 0.01688632369041443\n",
            "epoch: 955, Best Loss = 0.01688632369041443\n",
            "epoch: 956, Best Loss = 0.01688632369041443\n",
            "epoch: 957, Best Loss = 0.01688632369041443\n",
            "epoch: 958, Best Loss = 0.01688632369041443\n",
            "epoch: 959, Best Loss = 0.01688632369041443\n",
            "epoch: 960, Best Loss = 0.01688632369041443\n",
            "epoch: 961, Best Loss = 0.01688632369041443\n",
            "epoch: 962, Best Loss = 0.01688632369041443\n",
            "epoch: 963, Best Loss = 0.01688632369041443\n",
            "epoch: 964, Best Loss = 0.01688632369041443\n",
            "epoch: 965, Best Loss = 0.01688632369041443\n",
            "epoch: 966, Best Loss = 0.01688632369041443\n",
            "epoch: 967, Best Loss = 0.01688632369041443\n",
            "epoch: 968, Best Loss = 0.01688632369041443\n",
            "epoch: 969, Best Loss = 0.01688632369041443\n",
            "epoch: 970, Best Loss = 0.01617182046175003\n",
            "epoch: 971, Best Loss = 0.01617182046175003\n",
            "epoch: 972, Best Loss = 0.01617182046175003\n",
            "epoch: 973, Best Loss = 0.01617182046175003\n",
            "epoch: 974, Best Loss = 0.01617182046175003\n",
            "epoch: 975, Best Loss = 0.01617182046175003\n",
            "epoch: 976, Best Loss = 0.01617182046175003\n",
            "epoch: 977, Best Loss = 0.01617182046175003\n",
            "epoch: 978, Best Loss = 0.01617182046175003\n",
            "epoch: 979, Best Loss = 0.01617182046175003\n",
            "epoch: 980, Best Loss = 0.01617182046175003\n",
            "epoch: 981, Best Loss = 0.01617182046175003\n",
            "epoch: 982, Best Loss = 0.01617182046175003\n",
            "epoch: 983, Best Loss = 0.01617182046175003\n",
            "epoch: 984, Best Loss = 0.01617182046175003\n",
            "epoch: 985, Best Loss = 0.01617182046175003\n",
            "epoch: 986, Best Loss = 0.01617182046175003\n",
            "epoch: 987, Best Loss = 0.01617182046175003\n",
            "epoch: 988, Best Loss = 0.01617182046175003\n",
            "epoch: 989, Best Loss = 0.01617182046175003\n",
            "epoch: 990, Best Loss = 0.01617182046175003\n",
            "epoch: 991, Best Loss = 0.01617182046175003\n",
            "epoch: 992, Best Loss = 0.015495593659579754\n",
            "epoch: 993, Best Loss = 0.015495593659579754\n",
            "epoch: 994, Best Loss = 0.015495593659579754\n",
            "epoch: 995, Best Loss = 0.015495593659579754\n",
            "epoch: 996, Best Loss = 0.015495593659579754\n",
            "epoch: 997, Best Loss = 0.015495593659579754\n",
            "epoch: 998, Best Loss = 0.015495593659579754\n",
            "epoch: 999, Best Loss = 0.015495593659579754\n"
          ]
        }
      ],
      "source": [
        "best_loss = np.inf\n",
        "training_loss = []\n",
        "ode_loss = []\n",
        "ic_loss_list = []\n",
        "for epoch in range(num_epochs):\n",
        "  for x_batch, x0_batch, y0_batch in train_dataset_shuffled:\n",
        "    with tf.GradientTape() as t1:\n",
        "      loss_full, res_loss, ic_loss = loss_ode(model, x_batch, x0_batch, y0_batch)  # calculate losses\n",
        "      training_loss.append(loss_full.numpy())\n",
        "      ode_loss.append(res_loss.numpy())\n",
        "      ic_loss_list.append(ic_loss.numpy())\n",
        "    grad = t1.gradient(loss_full, model.trainable_variables)            # calculate gradients based off full loss\n",
        "    optimizer.apply_gradients(zip(grad, model.trainable_variables))     # apply gradients to optimizer\n",
        "    \n",
        "  if loss_full.numpy() < best_loss:\n",
        "    best_loss = loss_full.numpy()\n",
        "    model.save_weights('best_model')\n",
        "  \n",
        "  if epoch % 1 == 0:\n",
        "    #print(f'epoch: {epoch}, loss: {loss_full.numpy()}, res_loss: {res_loss.numpy()}, ic_loss:{ic_loss.numpy()}') \n",
        "    print(f'epoch: {epoch}, Best Loss = {best_loss}') \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(18,6))\n",
        "plt.plot(training_loss) \n",
        "plt.title('Training Loss')"
      ],
      "metadata": {
        "id": "zD7EV-EWp950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "6ede4555-d0cb-4b39-bcff-ce2a3c5a696b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Training Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z4jqqkS5zxOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "f980c7d9-cf0d-445e-9edc-c5180ad90ffb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# test the model \n",
        "x_test = np.linspace(0,2,100).reshape(100,1)\n",
        "x_test_tensor = tf.convert_to_tensor(x_test)\n",
        "\n",
        "model.load_weights('best_model')\n",
        "y_analytical = np.exp(-x_test**2)\n",
        "y_pred = model(x_test_tensor)\n",
        "\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "plt.plot(x_test, y_analytical, label='true solution')\n",
        "plt.plot(x_test, y_pred, label='predictions')\n",
        "plt.legend();"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ODE_solver_TF_20220725.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}